# ============================================================================
# Structured Random Projection (SRP) Subspace Merging Configuration
# Supports multiple merging functions including EMA with adaptive β_t
# ============================================================================

_target_: fusion_bench.method.srp_merging.SRPSubspaceMergeAlgorithm

# ============================================================================
# Core SRP Parameters
# ============================================================================
proj_ratio: 0.75              # Subspace compression ratio (0.0-1.0)
use_G: false                  # Use Gaussian scaling in FastFood transform
device: "cuda"                # Computation device
block_rows: 16384             # Memory management for large tensors

# ============================================================================
# Transform Type
# ============================================================================
transform_type: "srht"       # Options: srht | fwht | dct | dht | none | fastfood
                              # - srht: Subsampled Hadamard (default, recommended)
                              # - fwht: Full Hadamard (no subsampling)
                              # - dct: Discrete Cosine Transform
                              # - dht: Discrete Hartley Transform
                              # - none (or null): No projection, merge in original space (baseline)
                              # - fastfood: Original FastFood transform
# This parameter can be swept via --transform_types in the workflow/SLURM/CLI for fastfood_merging

# ============================================================================
# Adaptive Projection Size Estimation (Optional)
# ============================================================================
use_adaptive_proj_size: false # Enable adaptive projection size estimation
adaptive_proj_mode: "tensor"  # "tensor" | "layer" (scope for rank estimation and projection size computation)
                              # - tensor: per-tensor projection size (uses individual tensor dimensions)
                              # - layer: per-layer projection size (uses max dimension across layer tensors)
adaptive_proj_strategy: "rank" # "fixed" | "random" | "rank" | "layer_progressive" | "layer_group" | "layer_power_law"
                              # - fixed: m = ratio * d_last
                              # - random: m ~ U[m_min, f_max * d_last]
                              # - rank: m = beta * estimated_rank
                              # - layer_progressive: gradual scaling from start_ratio to end_ratio across layers
                              # - layer_group: fixed ratio per group (feature extraction vs head)
                              # - layer_power_law: power-law redistribution with global budget (B = global_ratio * sum(dims))
adaptive_proj_m_min: 8       # Minimum projection size
adaptive_proj_f_max: 1.0      # Maximum fraction of d_last
adaptive_proj_pow2: false      # Round to power-of-2
adaptive_proj_pow2_mode: "ceil" # "ceil" | "floor" | "nearest"
adaptive_proj_beta: 2.5       # For "rank" strategy: m = beta * estimated_rank
adaptive_proj_seed: null      # Random seed for "random" strategy (null = use global random)

# Layer Progressive Strategy Parameters (for adaptive_proj_strategy="layer_progressive")
adaptive_proj_start_ratio: 0.1  # Starting ratio (0.0-1.0) for first layer
adaptive_proj_end_ratio: 1.0    # Ending ratio (0.0-1.0) for last layer
adaptive_proj_growth_mode: "linear" # "linear" | "exponential"
                               # - linear: ratio(t) = start + t * (end - start)
                               # - exponential: ratio(t) = start * (end/start)^t
                               # where t ∈ [0,1] is normalized layer position
                               # Final projection size: m = ratio * d_last

# Layer Group Strategy Parameters (for adaptive_proj_strategy="layer_group")
adaptive_proj_group_boundary: 5    # Layer index separating feature extraction from head (0-based)
adaptive_proj_feature_ratio: 0.3   # FIXED projection ratio for feature extraction layers (0.0-1.0)
                               # Applied to layers 0 to boundary-1
adaptive_proj_head_ratio: 0.8      # FIXED projection ratio for head layers (0.0-1.0)
                               # Applied to layers >= boundary
                               # Note: These are FIXED ratios per group, not adaptive
                               # Final projection size: m = group_ratio * d_last
                               # Compatible with ALL adaptive_proj_mode and subspace_scope settings

# Global Power-Law Strategy Parameters (for adaptive_proj_strategy="global_power_law")
adaptive_proj_global_ratio: 0.25   # Overall compression budget (average ratio across all layers, 0.0-1.0)
                               # This maintains a global projection capacity budget B = global_ratio * sum(all_dims)
adaptive_proj_power_law_alpha: 0.85 # Power-law exponent for dimension-aware redistribution (0.7-0.95 typical)
                               # alpha < 1: Larger layers compressed more, smaller layers less (default: 0.85)
                               # alpha = 1: Uniform ratio (equivalent to fixed strategy)
                               # alpha > 1: Larger layers compressed less, smaller layers more
                               # Formula: m_i = B * (d_i^alpha) / sum(d_j^alpha)
                               # This ensures average compression = global_ratio while adapting to layer sizes

# ============================================================================
# Weight Matching Preprocessing (Optional - Git Re-Basin)
# ============================================================================
use_weight_matching: false    # Enable weight matching to align neurons before merging
weight_matching_max_iter: 100 # Maximum iterations for weight matching optimization
weight_matching_seed: 0       # Random seed for reproducibility
weight_matching_verbose: true # Print progress during weight matching
weight_matching_input_shapes: # Custom input shapes for permutation spec generation
  - [1, 3, 224, 224]          # Default: vision model (batch=1, RGB, 224x224)
                                # For NLP: [[1, 512]] or similar

# ============================================================================
# Subspace Configuration
# ============================================================================
subspace_scope: "per_tensor"  # per_tensor | per_flat_tensor | layer | global
                              # - per_tensor: row-wise projection (standard, recommended)
                              # - per_flat_tensor: flatten 2D to (out*in), one projection
                              # - layer: layer-wise shared projection
                              # - global: single global projection for all params
merge_where: "subspace"       # subspace | postlift

# ============================================================================
# Merging Strategy
# ============================================================================
merge_func: "signmax"         # Basic: sum | mean | max | signmax | random
                              # TIES: ties_sum | ties_mean | ties_max
                              # EMA: ema
                              # Advanced: energy_equalize | variance_aware | subspace_whiten | conflict_gating
                              #           elect_then_avg | soft_signmax | signmax_mad_normalized | align_weighted_mean
                              #           coherence_penalized | orthogonal_deflation | odm_ema_tuning_free
                              # TSV-Merge Inspired:
                              #   consensus_whiten: Best balance (keeps consensus, orthogonalizes conflicts)
                              #   spectral_denoise: Best for high noise / 10+ tasks (SVD filtering)
                              #   geometric_median: Best for outlier rejection (robust statistics)
                              #   stack_whiten_procrustes: Maximum interference reduction (SVD-based)
                              #   stack_whiten_newton: Same as procrustes but faster (SVD-free)
align_mode: "none"            # none | ties | tadrop

# ============================================================================
# EMA Parameters (Active when merge_func="ema")
# ============================================================================
ema_task_order: "custom"       # given | random | cosine_similarity | custom
ema_gamma: 2.0 #1.2                # Sigmoid scaling factor (0.5-2.0)
ema_w_c: 0.8 #0.6                  # Cosine alignment weight (0.0-1.0)
ema_w_s: 0.2 #0.4                  # Scale ratio weight (0.0-1.0, w_c + w_s = 1.0)
ema_custom_order: 
  - "SUN397"
  - "Stanford-Cars" 
  - "DTD"
  - "GTSRB"
  - "SVHN"
  - "MNIST"
  - "RESISC45"
  - "EuroSAT"

# ============================================================================
# TIES Merging Options (when merge_func starts with "ties_")
# Note: TIES implementation skips the Trim phase (no parameter pruning)
# and focuses on Elect phase (sign resolution) + Disjoint Merge phase
# ============================================================================
# ties_sum:  Elect majority sign per parameter, then sum agreeing task vectors
# ties_mean: Elect majority sign per parameter, then mean of agreeing task vectors  
# ties_max:  Elect majority sign per parameter, then max magnitude with correct sign

# ============================================================================
# Advanced Alignment Options
# ============================================================================
ties_trim_pct: 0.0            # TIES pruning percentage (0.0-1.0)
tadrop_tau: 0.0               # Task Arithmetic dropout threshold

# ============================================================================
# TSV-Style Linear/Non-Linear Separation
# ============================================================================
only_project_linear: false    # If true, only 2D tensors are projected; 1D tensors merged in original space
                              # This mimics TSV behavior: linear layers (weights) use subspace, non-linear (biases, norms) use original space

# Non-linear weight merging mode (active when only_project_linear=true)
nonlinear_merge_mode: "mean"  # mean | drop | sum | max | signmax
                              # - mean: Average of task vectors (original behavior, stable)
                              # - drop: Drop to zero (no update for non-linear params)
                              # - sum: Sum of task vectors with optional scaling
                              # - max: Max magnitude task vector
                              # - signmax: Majority sign, then max magnitude (TIES-like for non-linear)

nonlinear_scale: 1.0          # Scaling factor for non-linear weights (applies to all modes except drop)
                              # Useful with mode="sum" to control contribution

separate_norm_layers: false   # If true, normalization layers (BatchNorm, LayerNorm, etc.) are separated
                              # from other non-linear weights and merged using norm_merge_mode
                              # Non-norm non-linear weights use nonlinear_merge_mode

norm_merge_mode: "mean"       # mean | drop | sum | max | signmax (active when separate_norm_layers=true)
                              # Merging mode for normalization layers (BatchNorm, LayerNorm, GroupNorm, etc.)
                              # - mean: Average of task vectors (default, recommended for stability)
                              # - drop: Drop to zero (no update for norm params)
                              # - sum: Sum of task vectors with nonlinear_scale
                              # - max: Max magnitude task vector
                              # - signmax: Majority sign, then max magnitude

# ============================================================================
# Embedding Layer Control
# ============================================================================
project_embeddings: true      # If false, embedding layers are excluded from projection and merged in original space

# ============================================================================
# MLP Layer Control
# ============================================================================
drop_mlp_update: false        # If true, exclude MLP layers from task vector merging
                              # MLP weights will remain at pretrained values (no task updates applied)
                              # Applies to keys containing 'mlp', 'fc', 'ffn', 'feed_forward', 'c_fc', 'c_proj'
                              # Embedding layers are detected by name patterns: position_embedding, pos_embed, patch_embed, 
                              # token_embedding, cls_token, mask_token, decoder_embed, text_projection, etc.
                              # Use case: Embeddings often have distinct representations that may not benefit from projection
                              # This parameter is complementary to only_project_linear
                              # - only_project_linear=false + project_embeddings=false: Embeddings merged in original space
                              # - only_project_linear=true + project_embeddings=false: Embeddings merged in original space (if 2D)
                              # Default: true (all layers projected, maintains backward compatibility)

# ============================================================================
# Post-processing Options
# ============================================================================
use_pareto: false             # Apply Pareto frontier optimization
use_rescale: false            # Rescale merged parameters
weights: null                 # Task importance weights [w1, w2, ..., wN]
scale: 1.0                    # Global scaling factor

# ============================================================================
# Decision Tracking Options (for max/signmax aggregation)
# ============================================================================
log_max_decision: false      # Track which task wins how many decisions (absolute and relative)
                             # Only active when merge_func is "max" or "signmax"
                             # Tracks wins at layer level and globally
                             # Results saved to decision_statistics.json and included in output files

# ============================================================================
# Integrated Analysis Options
# ============================================================================
run_analysis: false          # Whether to run analysis after merging (disabled for SLURM workflow control)
analysis_methods: [merged_task_vector, task_vector_similarity, task_vector_layer]
          # List of analysis methods to run
                             # Options: merged_task_vector, task_vector_similarity, task_vector_layer
analysis_output_path: null    # Path for analysis outputs (uses fabric logger if null)

# ============================================================================
# Task Arithmetic Reconstruction mode
# ============================================================================
use_task_arithmetic_reconstruction: false
task_arithmetic_scaling: 1.0  # Standard TA scaling factor
report_reconstruction_error: true  # Report reconstruction error after projection

# ============================================================================
# LiNeS (Layer Scaling) Parameters
# ============================================================================
use_lines: false              # Enable LiNeS layer-wise progressive scaling
lines_num_blocks: null        # Number of residual blocks (null = auto-detect: 12 for ViT-B, 24 for ViT-L)
lines_alpha: null             # Minimum scaling factor (null = auto-compute or use beta)
                              # - If lines_auto_alpha=true: alpha = (norm_summed_tvs / norm_merged_tv) * (1 / num_tasks)
                              # - If lines_auto_alpha=false and lines_alpha=null: alpha = lines_beta
lines_beta: 1.0               # Maximum additional scaling (creates gradient from early to late layers)
                              # Scaling formula: scale(layer) = alpha + beta * (layer_idx / (num_blocks - 1))
                              # Example: alpha=0.2, beta=0.8 → scales from 0.2 (early layers) to 1.0 (late layers)
lines_auto_alpha: true        # Auto-compute alpha from task vector norms (recommended for multi-task)
                              # When true: alpha adapts to the merging configuration
                              # When false: use manual lines_alpha or default to lines_beta

# LiNeS Explanation:
# - LiNeS applies layer-depth-dependent scaling to merged task vectors
# - Early layers (general features): lower scaling → preserve pretrained knowledge
# - Later layers (task-specific): higher scaling → preserve task adaptations
# - This reduces catastrophic forgetting and improves multi-task merging
# - Compatible with all merge functions (sum, mean, ties, ema, signmax, etc.)
# - Reference: "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging"

# ============================================================================
# Multi-Sketch Parameters
# ============================================================================
num_sketches: 1               # Number of independent random projections (J ≥ 1)
                              # - 1: Standard single-sketch merging (default)
                              # - >1: Multi-sketch ensemble merging
                              # Multi-sketching uses J independent random projections A_1, ..., A_J
                              # Each sketch captures a different random subspace of dimension m < d
                              # Benefits:
                              # - Single projection loses information in its nullspace
                              # - Multiple sketches with different random seeds cover more of full space
                              # - Ensemble reconstruction reduces information loss
                              # - Computational cost: O(J × merging_cost)
                              # Recommended: 3-5 sketches for balance between quality and speed
sketch_ensemble_mode: "mean"  # How to combine lifted sketches: mean | sum | max | median
                              # - mean: Average across sketches (recommended, reduces variance)
                              # - sum: Sum across sketches (amplifies signal)
                              # - max: Take maximum magnitude (preserves peaks)
                              # - median: Robust to outlier sketches
                              # Applied after lifting each sketch back to original space

# ============================================================================
# Subspace Boosting (SB) Parameters
# ============================================================================
use_subspace_boosting: false  # Enable Subspace Boosting to restore rank to collapsed task-vectors
                              # When merging many experts, task-vector matrices often collapse in rank
                              # with most energy concentrated in a few singular directions
                              # SB boosts neglected singular values to restore fuller-rank updates
                              # Applied AFTER merging, only to linear layers (MLP + attention weights)
                              # Compatible with all merge functions (sum, mean, ties, ema, signmax, etc.)
subspace_boosting_beta: 0.01  # Cumulative energy threshold (0.0-1.0)
                              # Defines how much cumulative singular value energy to keep untouched
                              # Typical range: 0.00-0.02
                              # - Small beta (e.g., 0.00): Very aggressive boosting (more rank restored)
                              # - Medium beta (e.g., 0.01): Balanced boosting (recommended)
                              # - Large beta (e.g., 0.05): Mild boosting
                              # Algorithm:
                              #   1. SVD: T = U Σ V^T
                              #   2. Find k* where cumulative_energy(k*) >= beta
                              #   3. Boost: σ_i' = max(σ_i, σ_k*)
                              #   4. Reconstruct: T' = U Σ' V^T
                              # Reference: "Subspace Boosting for Model Merging"

# ============================================================================
# Consensus Masking (TALL Masks) Parameters
# ============================================================================
use_consensus_mask: false     # Enable consensus masking to filter merged parameters
                              # Uses TALL masks (Task-Adaptive Low-rank Learning) to identify
                              # important parameters per task, then keeps only parameters that
                              # multiple tasks agree are important
                              # Applied AFTER merging (and Subspace Boosting if enabled)
                              # Compatible with all merge functions
                              # Algorithm:
                              #   1. Generate per-task masks: mask_t = |θ_0 - θ_t| > |θ_merged - θ_t| * λ
                              #   2. Count cross-task agreement: count = Σ mask_t
                              #   3. Keep only if count >= consensus_threshold
                              # Removes:
                              #   - Catastrophic weights (not used by any task)
                              #   - Selfish weights (used by only one task)
                              # Reference: "Task Arithmetic in the Tangent Space" (Ilharco et al., 2023)
tall_mask_lambda: 0.6         # Distance threshold for per-task mask generation (0.0-1.0)
                              # Mask criterion: |θ_0 - θ_t| > |θ_merged - θ_t| * λ
                              # Intuition: Keep parameters where task made large changes from pretrained
                              # but merged model is close to task model
                              # Typical range: 0.2-0.8
                              # - Smaller λ (e.g., 0.2): More permissive (more parameters activated)
                              # - Larger λ (e.g., 0.8): Stricter (fewer parameters activated)
consensus_threshold: 2        # Minimum number of tasks that must activate a parameter (>= 0)
                              # For each parameter position:
                              #   count = number of tasks with mask=1
                              #   keep parameter if count >= threshold
                              # Typical values:
                              # - 0: No filtering (equivalent to disabling consensus mask)
                              # - 1: Remove catastrophic weights only (not used by any task)
                              # - 2: Remove catastrophic + selfish weights (default, recommended)
                              # - 3+: Increasingly strict (only keep highly shared parameters)
                              # - >num_tasks: Remove everything (not useful)

# ============================================================================
# Iso-Merging Parameters (Isotropic Model Merging)
# ============================================================================
use_iso_preprocessing: false  # Apply Iso-C scaling to each task vector BEFORE projection/merging
                              # Iso-C makes singular value spectrum uniform for each task vector
                              # Applied individually: τ_i = Iso(θ_i - θ_base) for each task i
                              # Then task vectors are projected and merged as usual
                              # Compatible with all merge functions and projection modes
use_iso_postprocessing: false # Apply Iso-C scaling to merged task vector AFTER merging
                              # Makes the final merged task vector's spectrum uniform
                              # Applied to: τ_merged = Iso(merged task vector)
                              # Then applied to base: θ_final = θ_base + τ_merged
                              # Applied BEFORE Subspace Boosting, LiNeS, and Consensus Mask
                              # Compatible with all merge functions
# Note: Both can be enabled simultaneously for double Iso-C application
# Iso-C Algorithm (from "No Task Left Behind" paper):
#   1. Compute SVD: T = U Σ V^T
#   2. Replace singular values with mean: Σ' = mean(Σ) * I
#   3. Reconstruct: T' = U Σ' V^T
# This prevents a few dominant singular values from dominating the merge
# Reference: Marczak et al., "No Task Left Behind: Isotropic Model Merging
#            with Common and Task-Specific Subspaces", ICML 2025
